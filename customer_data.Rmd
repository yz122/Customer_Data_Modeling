---
title: "Customer Data Project"
output: html_document
date: "2024-04-04"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Exploration

## Load Packages and Data

```{R, load packages}
library(tidyverse)

library(coefplot)

library(glmnet)

library(ggplot2)



library(boot)

library(caret)

library(AppliedPredictiveModeling)

library(reshape2)

library(Rcpp)

library(rstanarm)

library(magrittr) 

library(dplyr)

```


```{r, read_data}
df_train <- readr::read_csv("paint_project_train_data.csv", col_names = TRUE)

df_holdout <- readr::read_csv("paint_project_holdout_data.csv", col_names = TRUE)

df_bonus <- readr::read_csv("paint_project_bonus_data.csv", col_names = TRUE)

df_train %>% glimpse()

```





## Data Processing

Use boot::logit for response

```{R}
#min_value <- min(df_train$response, na.rm = TRUE) 
#max_value <- max(df_train$response, na.rm = TRUE) 

min_value <- 0
max_value <- 100

# Normalize to the (0, 1) range
train_response_normalized <- (df_train$response - min_value) / (max_value - min_value)


df_train$response_logit <- boot::logit(train_response_normalized)

#df_train$response_logit 



# Applying the inverse logit
#df_train$response_back <- boot::inv.logit(df_train$response_logit)

# Denormalize if necessary
#df_train$response_original_scale <- df_train$response_back * (max_value - min_value) + min_value

df_train %>% head()

```




## Visualization


Counts for categorical variables.

```{R}
# levels
df_train %>%  tail()

df_train$Lightness %>% unique()
df_train$Saturation %>% unique()

```


We start by plotting discrete variables:
```{R}

ggplot(df_train, aes(x = factor(Lightness))) + 
  geom_bar(fill = "red", color = "black") +
  ggtitle("Counts")


ggplot(df_train, aes(x = factor(Saturation))) + 
  geom_bar(fill = "red", color = "black") +
  ggtitle("Counts")

ggplot(df_train, aes(x = factor(outcome))) + 
  geom_bar(fill = "red", color = "black") +
  ggtitle("Counts")

```
We note that Lightness and Saturation have fairly uniform distribution. The loss of "gray" in Saturation is likely due to aesthetic or practical reasons.


Then continuous variables, including RGB, Hue and response values.
```{R}

continuous_vars <- c("R", "G", "B", "Hue", "response_logit")
desired_bins <- 30
for(var in continuous_vars) {
  var_range <- range(df_train[[var]], na.rm = TRUE)

  binwidth_var <- (var_range[2] - var_range[1]) / desired_bins

  print(
    ggplot(df_train, aes_string(x = var)) +
    geom_histogram(binwidth = binwidth_var, fill = "skyblue", color = "black") +
    ggtitle(paste("Distribution of", var))
  )
}



```
None of the continuous variables follows Gaussian-like distribution.


## Continuous Inputs conditioned on the Discrete Inputs 


```{R}
# Lightness
continuous_vars <- c("R", "G", "B", "Hue", "response_logit")

for(var in continuous_vars) {

  var_range <- range(df_train[[var]], na.rm = TRUE)
  desired_bins <- 30
  binwidth_var <- (var_range[2] - var_range[1]) / desired_bins
  
  # Plot with faceting by 'Lightness'
print(
    ggplot(df_train, aes_string(x = var)) +
    geom_histogram(aes(y = ..density..), binwidth = binwidth_var, fill = "skyblue", color = "black") +
    geom_density(alpha = 0.2, fill = "red") +
    facet_wrap(~ Lightness) +
    theme_minimal() +
    ggtitle(paste( var, "vs. Levels of Lightness"))
  )
}

```
Given different levels of light, especially lighter shades, continuous _GRB_ and _logit response_ variable follows obvious distribution.

It is almost uniform for Hue, however, when levels of Lightness applied. This is reasonable, since in this metric, Hue and Lightness are suppposed to be independent with each other.

```{R}
# Saturation
continuous_vars <- c("R", "G", "B", "Hue", "response_logit")

for(var in continuous_vars) {
  # Dynamically adjust binwidth 
  var_range <- range(df_train[[var]], na.rm = TRUE)
  desired_bins <- 30
  binwidth_var <- (var_range[2] - var_range[1]) / desired_bins
  
  # Plot with faceting by 'Saturation'
  print(
    ggplot(df_train, aes_string(x = var)) +
    geom_histogram(aes(y = ..density..), binwidth = binwidth_var, fill = "skyblue", color = "black") +
    geom_density(alpha = 0.2, fill = "red") +
    facet_wrap(~ Saturation) +
    theme_minimal() +
    ggtitle(paste( var, "vs. Levels of Saturation"))
  )
}

```
On the other hand, Saturation has less influence on continuous variables compared to Lightness. We observe some trends on RGB, but more diffusive. 


```{R}
# Outcome
continuous_vars <- c("R", "G", "B", "Hue", "response_logit")

for(var in continuous_vars) {
  # Dynamically adjust binwidth 
  var_range <- range(df_train[[var]], na.rm = TRUE)
  desired_bins <- 30
  binwidth_var <- (var_range[2] - var_range[1]) / desired_bins
  
  # Plot with faceting by 'Outcome'
  print(
    ggplot(df_train, aes_string(x = var)) +
    geom_histogram(aes(y = ..density..), binwidth = binwidth_var, fill = "skyblue", color = "black") +
#     geom_boxplot(fill = "red", color = "black") +
      geom_density(alpha = 0.2, fill = "red") +
    facet_wrap(~ outcome) +
    theme_minimal() +
    ggtitle(paste( var, "vs. Levels of Outcome"))
  )
}

```
The difference between levels on outcome is not strong.


## Continuous Inputs vs. Continuous Inputs


We use ``caret::featurePlot`` to visualize the relation between coninuous inputs.
```{R}
#continuous_vars <- c("R", "G", "B", "Hue", "response_logit")
continuous_data <- df_train[ c("R", "G", "B", "Hue")]

# Ensure 'outcome' is a factor

#df_train$outcome <- as.factor(df_train$outcome)


# Including 'outcome' for coloring in the plot
featurePlot(x = continuous_data,
            y = as.factor(df_train$outcome),
            plot = "pairs", alpha = 0.7,
            auto.key = list(columns = 2))



```



To study the correlation among the continuous inputs RBG and Hue, we need to compute the covariance matrix.

```{R}
continuous_vars <- c("R", "G", "B", "Hue", "response_logit")


df_train_cor <-  df_train %>%dplyr::select(continuous_vars) %>% cor()

df_train_cor 

#highCorr <- sum(abs(df_train_cor[upper.tri(df_train_cor)]) > .7)
#highCorr


#Convert the correlation_matrix into a long format suitable for ggplot
melted_correlation_matrix <- melt(df_train_cor)

ggplot(data = melted_correlation_matrix, aes(x = Var1, y = Var2, fill = value)) + 
  geom_tile() + 
  scale_fill_gradient2(low = "black", high = "red", mid = "white", midpoint = 0) +
  ggtitle("Correlation Matrix of Continuous Variables")
```
Note that G and response_logit have pretty high correlations. Other pairs are less so.




## Continuous Inputs vs. Continuous Outputs


```{R}
continuous_vars <- c("R", "G", "B", "Hue", "response_logit")
continuous_data <- df_train[ c("R", "G", "B", "Hue", "response_logit")]

# Ensure 'outcome' is a factor
df_train$outcome <- as.factor(df_train$outcome)


for(var in  c("R", "G", "B", "Hue")){
  print(
    ggplot(df_train, aes_string(x = var, y = 'response_logit', color = 'outcome')) +
    geom_point(alpha = 0.8, size = 3) +
    geom_smooth() +
    theme_minimal() +
    labs(title =  "Continuous Variable vs. response_logit with Outcome levels")
    
  )
}
```
We see a clear correlation between RGB and ``response_logit``, among which ``G`` is the strongest. It is less so from ``Hue``.  From the pairs with correlations, he trends depend on the categorical outputs, ``outcome``.


To further investigate the influence of categorical inputs, we repeat the code above.
```{R}
# Saturation
for(var in  c("R", "G", "B", "Hue")){
  print(
    ggplot(df_train, aes_string(x = var, y = 'response_logit')) +
      geom_point(alpha = 0.8, size = 1, color = "red") +
      facet_wrap(~ Saturation) +
      theme_minimal() +
      labs(title =  "Continuous Variable vs. response_logit with Saturation Levels")
  )
}
```
Again, ``G`` is the most dominant variable while ``RGB`` all contributes. Trends do depend on the levels of Saturation; for some inputs like ``grey``, ``neutral`` and ``shaded``, it is more significant. For ``Hue``, however, it showed no obvious trend.

```{R}
# Lightness
for(var in  c("R", "G", "B", "Hue")){
  print(
    ggplot(df_train, aes_string(x = var, y = 'response_logit')) +
      geom_point(alpha = 0.8, size = 1, color = "red") +
      facet_wrap(~ Lightness) +
      theme_minimal() +
      labs(title =  "Continuous Variable vs. response_logit with Lightness Levels")
  )
}


```
The results for levels of ``Lightness`` agrees with previous findings mostly. Only one difference is ``Hue`` actually started to show trends with respect to levels of ``Lightness``.


## Continuous/Discrete Inputs vs Discrete Outputs

To visualize the behavior of the binary outcome with respect to the continuous inputs and to visualize the behavior of the binary outcome with respect to the categorical inputs, we make the plot of continuous variables vs ``outcome`` and facet wrap them with respect to the discrete variables. 

Note that we used ``geom_jitter()`` to avoid overlapping and to improve visualization.



```{R}
# Saturation

df_train <- readr::read_csv("paint_project_train_data.csv", col_names = TRUE)

min_value <- 0
max_value <- 100

# Normalize to the (0, 1) range
train_response_normalized <- (df_train$response - min_value) / (max_value - min_value)
df_train$response_logit <- boot::logit(train_response_normalized)
df_train_cla <-dplyr::select(df_train, -response_logit, -response)




var <- "R"
deg <- 1
df_train %>%
    ggplot(aes_string(x = var, y = "outcome")) +  
    geom_jitter(height = 0.02, width = 0, color = "red") +
    geom_smooth(formula = y ~ poly(x, deg), method = "glm", 
                method.args = list(family = "binomial"), color = "blue") +
    facet_wrap(~Saturation) +
    labs(title = paste("Logistic Regression Fit for", var, "vs. Outcome")) +
    theme_minimal() 

  
var <- "G"
deg <- 1
df_train %>%
    ggplot(aes_string(x = var, y = "outcome")) +  
    geom_jitter(height = 0.02, width = 0, color = "red") +
    geom_smooth(formula = y ~ poly(x, deg), method = "glm", 
                method.args = list(family = "binomial"), color = "blue") +
    facet_wrap(~Saturation) +
    labs(title = paste("Logistic Regression Fit for", var, "vs. Outcome")) +
    theme_minimal() 

var <- "B"
deg <- 1
df_train %>%
    ggplot(aes_string(x = var, y = "outcome")) +  
    geom_jitter(height = 0.02, width = 0, color = "red") +
    geom_smooth(formula = y ~ poly(x, deg), method = "glm", 
                method.args = list(family = "binomial"), color  = 'blue') +
    #facet_wrap(~Saturation) +
    labs(title = paste("Logistic Regression Fit for", var, "vs. Outcome")) +
    theme_minimal() 


var <- "Hue"
deg <- 2
df_train %>%
    ggplot(aes_string(x = var, y = "outcome")) +  
    geom_jitter(height = 0.02, width = 0, color = "red") +
    geom_smooth(formula = y ~ poly(x, deg), method = "glm", 
                method.args = list(family = "binomial"), color = "blue") +
    facet_wrap(~Saturation) +
    labs(title = paste("Logistic Regression Fit for", var, "vs. Outcome")) +
    theme_minimal() 

```


```{R}
# Lightness


#for (var in c("R", "G", "B", "Hue")) {}
var <- "R"
deg <- 1
df_train %>%
    ggplot(aes_string(x = var, y = "outcome")) +  
    geom_jitter(height = 0.02, width = 0, color = "red") +
    geom_smooth(formula = y ~ poly(x, deg), method = "glm", 
                method.args = list(family = "binomial"), color = "blue") +
    facet_wrap(~Lightness) +
    labs(title = paste("Logistic Regression Fit for", var, "vs. Outcome")) +
    theme_minimal() 

  
var <- "G"
deg <- 1
df_train %>%
    ggplot(aes_string(x = var, y = "outcome")) +  
    geom_jitter(height = 0.02, width = 0, color = "red") +
    geom_smooth(formula = y ~ poly(x, deg), method = "glm", 
                method.args = list(family = "binomial"), color = "blue") +
    facet_wrap(~Lightness) +
    labs(title = paste("Logistic Regression Fit for", var, "vs. Outcome")) +
    theme_minimal() 

var <- "B"
deg <- 1
df_train %>%
    ggplot(aes_string(x = var, y = "outcome")) +  
    geom_jitter(height = 0.02, width = 0, color = "red") +
    geom_smooth(formula = y ~ poly(x, deg), method = "glm", 
                method.args = list(family = "binomial"), color = "blue") +
    facet_wrap(~Lightness) +
    labs(title = paste("Logistic Regression Fit for", var, "vs. Outcome")) +
    theme_minimal() 


var <- "Hue"
deg <- 2
df_train %>%
    ggplot(aes_string(x = var, y = "outcome")) +  
    geom_jitter(height = 0.02, width = 0, color = "red") +
    geom_smooth(formula = y ~ poly(x, deg), method = "glm", 
                method.args = list(family = "binomial"), color = "blue") +
    facet_wrap(~Lightness) +
    labs(title = paste("Logistic Regression Fit for", var, "vs. Outcome")) +
    theme_minimal() 
```


Given levels of discrete variable, the all continuous variable including ``Hue`` showed relatively strong trends
with respect to ``outcome``. For ``glm`` model, our suggested degree for R, G, B and Hue are 1, 1, 1, 2





# Regression 
## Linear Models

We plan to train _10 different models_ using ``lm()``:

1. Intercept-only model – no INPUTS!

2. Categorical variables only – linear additive

3. Continuous variables only – linear additive

4. All categorical and continuous variables – linear additive

5. Interaction of the categorical inputs with all continuous inputs main effects

6. Add categorical inputs to all main effect and all pairwise interactions of continuous inputs

7. Interaction of the categorical inputs with all main effect and all pairwise interactions of continuous inputs


8. Try non-linear basis functions based on your EDA. Guess: (R^2 + G + B) * (Lightness + 1)
  
9. Can consider interactions of basis functions with other basis functions!

10. Can consider interactions of basis functions with the categorical inputs!



First, we'll fit several linear models. Starting from simple (intercept only) to more complex models with interactions and possibly non-linear transformations (basis functions).


Note that we start by splitting the training data to training and testing. This reduce the overfitting while applying our metric.

```{R}

set.seed(123)
inTraining <- createDataPartition(df_train$response_logit, p=.95, list = FALSE)
training_reg <- df_train[inTraining,]
testing_reg  <- df_train[-inTraining,]


# Intercept-only model – no INPUTS!
mod_reg_1 <- lm(response_logit ~ 1, data = training_reg)
#mod_reg_1 <- lm(response_logit ~ (Lightness + Saturation + Hue) * (R + G + B), data = training_reg)

# Categorical variables only – linear additive
mod_reg_2 <- lm(response_logit ~ Lightness + Saturation, data = training_reg)

# Continuous variables only – linear additive
mod_reg_3 <- lm(response_logit ~ R + G + B + Hue, data = training_reg)

# All categorical and continuous variables – linear additive
mod_reg_4 <- lm(response_logit ~ Lightness + Saturation + R + G + B + Hue, data = training_reg)

# Interaction of the categorical inputs with all continuous inputs main effects
mod_reg_5 <- lm(response_logit ~ (Lightness + Saturation) * (R + G + B + Hue), data = training_reg)

# Add categorical inputs to all main effect and all pairwise interactions of continuous inputs
mod_reg_6 <- lm(response_logit ~ Lightness + Saturation + R + G + B + Hue 
                + R:G + R:B + R:Hue + G:B + G:Hue + B:Hue, data = training_reg)

# Interaction of the categorical inputs with all 
# main effect and all pairwise interactions of continuous inputs
mod_reg_7 <- lm(response_logit ~ (Lightness + Saturation) *
                  (R + G + B + Hue + R:G + R:B + R:Hue + G:B + G:Hue + B:Hue), data = training_reg)

#  Try non-linear basis functions based on your EDA
#mod_reg_8 <- lm(response_logit ~  (R + I(R^2) + G + B) * (Lightness * Hue + Hue * Saturation), data = training_reg)
mod_reg_8 <- lm(response_logit ~  (poly(R, 2) + poly(G, 2) + poly(B, 2) + poly(Hue, 2)) * (Lightness + Saturation), data = training_reg)


# Can consider interactions of basis functions with other basis functions!
mod_reg_9 <- lm(response_logit ~  (poly(R, 2) * poly(G, 2) + poly(B, 2) * poly(Hue, 2) + poly(R, 2) * poly(Hue, 2) + poly(B, 2) * poly(R, 2)  + poly(G, 2) * poly(Hue, 2) + poly(B, 2) * poly(G, 2))
                , data = training_reg)

# Can consider interactions of basis functions with the categorical inputs!
mod_reg_10 <- lm(response_logit ~  (Lightness + Saturation) * (poly(R, 2) * poly(G, 2) + poly(B, 2) * poly(Hue, 2) + poly(R, 2) * poly(Hue, 2) + poly(B, 2) * poly(R, 2) + poly(G, 2) * poly(Hue, 2) + poly(B, 2) * poly(G, 2))
                , data = training_reg)

```


## Performance metric



Now we visualize the coefficient summaries for your top 3 models by root mean square error(RMSE) metric and R-squared metric.

```{R}
library(modelr)

# test

# RMSE
e1 <-rmse(mod_reg_1, testing_reg )
e2 <-rmse(mod_reg_2, testing_reg )
e3 <-rmse(mod_reg_3, testing_reg )
e4 <-rmse(mod_reg_4, testing_reg )
e5 <-rmse(mod_reg_5, testing_reg )
e6 <-rmse(mod_reg_6, testing_reg )
e7 <-rmse(mod_reg_7, testing_reg )
e8 <-rmse(mod_reg_8, testing_reg )
e9 <-rmse(mod_reg_9, testing_reg )
e10 <-rmse(mod_reg_10, testing_reg )
RMSE_errors <- data.frame(x = seq(1, 10), 
                     y = c(e1, e2, e3, e4, e5, e6, e7, e8, e9, e10))

RMSE_errors %>% ggplot( aes(x = x, y = y)) + 
  geom_point() + 
  geom_line(color = "red", size = 1) + 
  labs(title = "RMSE errors", x = "Model", y = "Error")
```

```{R}
# R-squared

r_squared_1 <-rsquare(mod_reg_1, testing_reg )
r_squared_2 <-rsquare(mod_reg_2, testing_reg )
r_squared_3 <-rsquare(mod_reg_3, testing_reg )
r_squared_4 <-rsquare(mod_reg_4, testing_reg )
r_squared_5 <-rsquare(mod_reg_5, testing_reg )
r_squared_6 <-rsquare(mod_reg_6, testing_reg )
r_squared_7 <-rsquare(mod_reg_7, testing_reg )
r_squared_8 <-rsquare(mod_reg_8, testing_reg )
r_squared_9 <-rsquare(mod_reg_9, testing_reg )
r_squared_10 <-rsquare(mod_reg_10, testing_reg )

rsquare_errors <- data.frame(x = seq(1, 10), 
                     y = c(r_squared_1, r_squared_2, r_squared_3, r_squared_4, r_squared_5, r_squared_6, r_squared_7, r_squared_8, r_squared_9, r_squared_10))

rsquare_errors %>% ggplot(aes(x = x, y = y)) + 
  geom_point() + 
  geom_line(color = "red", size = 1) + 
  labs(title = "R-squared", x = "Model", y = "Error")
```

The best 3 models according both R-squared and RMSE metric are model 9, 8 and 7. We have the visualization as the following.

```{R}
# determine the best models
RMSE_errors[order(RMSE_errors$y),]
rsquare_errors[order(rsquare_errors$y),]


# coefplot
coefplot(mod_reg_9) + labs(title = 'Coefficient Plot for Model 9')

coefplot(mod_reg_8) + labs(title = 'Coefficient Plot for Model 8')

coefplot(mod_reg_7) + labs(title = 'Coefficient Plot for Model 7')

#mod_reg_7
#coefplot::multiplot(mod_reg_7, mod_reg_8, mod_reg_9) +
#  labs(title = 'Coefficient Plot for Model 7, 8 and 9')
```



Note that the naive plotting function give messy results, then we find the significant coefficients by selecting the ones whose confidence interval does not intersect 0. To avoid noises, we designed a cutoff to eliminate significant values that contribute little. 
```{R}

#coef_summary <- summary(mod_reg_10)$coefficients
#coef_summary[coef_summary[, "Pr(>|t|)"] < 0.05, ] %>% print()
cutoff <- 0.01
ci_9 <- confint(mod_reg_9)
sig_9 <- ci_9[!(ci_9[,1] <= 0 & ci_9[,2] >= 0) & (abs(ci_9[,1] + ci_9[,2])/2 > cutoff), ]
#print("Model 9")
#print(sig_9)

sig_9t <-  sig_9 %>% as.data.frame() %>%
  mutate(Estimate = (`2.5 %` + `97.5 %`) / 2,
         lower_bound = `2.5 %` * 1,
         upper_bound =  `97.5 %`
         )  %>%  rownames_to_column()

sig_9t %>%  ggplot( aes(x = rowname, y = Estimate, ymin = lower_bound, ymax = upper_bound)) +
  geom_pointrange() +  
  coord_flip() +  
  labs(title = "Significant Coefficients Model 9", x = "Term", y = "Estimate") +
  theme_minimal() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red")  







cutoff <- 1
ci_8 <- confint(mod_reg_8)
sig_8 <- ci_8[!(ci_8[,1] <= 0 & ci_8[,2] >= 0)  & (abs(ci_8[,1] + ci_8[,2])/2 > cutoff), ]
#print("Model 8")
#print(sig_8)


sig_8t <-  sig_8 %>% as.data.frame() %>%
  mutate(Estimate = (`2.5 %` + `97.5 %`) / 2,
         lower_bound = `2.5 %` * 1,
         upper_bound =  `97.5 %`
         )  %>%  rownames_to_column()

sig_8t %>%  ggplot( aes(x = rowname, y = Estimate, ymin = lower_bound, ymax = upper_bound)) +
  geom_pointrange() +  
  coord_flip() +  
  labs(title = "Significant Coefficients Model 8", x = "Term", y = "Estimate") +
  theme_minimal() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red")  






cutoff <- 0.01
ci_7 <- confint(mod_reg_7)
sig_7 <- ci_7[!(ci_7[,1] <= 0 & ci_7[,2] >= 0) & (abs(ci_7[,1] + ci_7[,2])/2 > cutoff), ]
#print("Model 7")
#print(sig_7)



sig_7t <- sig_7 %>% as.data.frame() %>%
  mutate(Estimate = (`2.5 %` + `97.5 %`) / 2,
         lower_bound = `2.5 %` * 1,
         upper_bound =  `97.5 %`
         )  %>%  rownames_to_column()

sig_7t %>%  ggplot( aes(x = rowname, y = Estimate, ymin = lower_bound, ymax = upper_bound)) +
  geom_pointrange() +  
  coord_flip() +  
  labs(title = "Significant Coefficients Model 7", x = "Term", y = "Estimate") +
  theme_minimal() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red")  






```
``Lightness`` is a key factor. Especially in model 7, where Lightness, or the interaction with it, dominated every significant coefficient. Second to lightness, ``Saturation`` also  dominated in model  8  and 9.




## Bayesian Linear models



We will fit model 9 and model 8. Model 9 is the obvious choice as our best performer. Model 8 comes from our EDA and its performance is almost the same as model 10 but with less complexity. 

Here we will use ``library(rstanarm)`` and ``stan_lm() ``.



```{R}

# Recall our models
# mod_reg_8 <- lm(response_logit ~  (poly(R, 2) + poly(G, 2) + poly(B, 2) + poly(Hue, 2)) * (Lightness + Saturation), data = training_reg)

#mod_reg_9 <- lm(response_logit ~  (poly(R, 2) * poly(G, 2) + poly(B, 2) * poly(Hue, 2) + poly(R, 2) * poly(Hue, 2) + poly(B, 2) * poly(R, 2)  + poly(G, 2) * poly(Hue, 2) + poly(B, 2) * poly(G, 2))
#                , data = training_reg)



#model_reg_7_B <- stan_lm(response_logit ~  (Lightness + Saturation) *
#                         (R + G + B + Hue + R:G + R:B + R:Hue + G:B + G:Hue + B:Hue), 
#                        data = training, 
#                        prior = R2(location = 0.5),  
#                        seed = 123) 
#mod_reg_7 <- lm(response_logit ~ (Lightness + Saturation) *
#                  (R + G + B + Hue + R:G + R:B + R:Hue + G:B + G:Hue + B:Hue), data = df_train)



library(rstanarm)

df_train_B <- df_train %>% mutate(
  R2 = R^2,
  G2 = G^2,
  B2 = B^2,
  Hue2 = Hue^2
  )


set.seed(123)
inTraining <- createDataPartition(df_train_B$response_logit, p=.5, list = FALSE)
training <- df_train_B[inTraining,]
testing  <- df_train_B[-inTraining,]
```

```{R, eval=FALSE}
model_reg_8_B <- stan_lm(response_logit ~ (R + G + B + R2 + G2 + B2 + Hue + Hue2) * (Lightness + Saturation), 
                        data = training, 
                        prior = R2(location = 0.5),  
                        seed = 123) 

#model_reg_8_B_permanant <- model_reg_8_B

# Save Model
model_reg_8_B %>% readr::write_rds("model_reg_8_B.rds")
#re_load_model_reg_8_B <- readr::read_rds("model_reg_8_B.rds")


model_reg_9_B <- stan_lm(response_logit ~  (  (R+R2) * ((B+B2)+(G+G2)+(Hue + Hue2)) + 
                                                                     ((B+B2) * ((G+G2)+(Hue + Hue2)) +
                                                                     (G+G2) *(Hue + Hue2) ) ),
                                  data = training, 
                                  prior = R2(location = 0.5),  
                                  seed = 123)  

#model_reg_9_B_permanant <- model_reg_9_B

# Save Model
model_reg_9_B %>% readr::write_rds("model_reg_9_B.rds")
#re_load_model_reg_9_B <- readr::read_rds("model_reg_9_B.rds")
```

```{R}
model_reg_8_B <- readr::read_rds("model_reg_8_B.rds")
model_reg_9_B <- readr::read_rds("model_reg_9_B.rds")

# performance  metric
rsquare(model_reg_8_B, data = testing)
rsquare(model_reg_9_B, data = testing)


rmse(model_reg_8_B, data = testing)
rmse(model_reg_9_B, data = testing)



```
From both R-squared and RMSE, model 9 has slightly better performance.




```{R}
#library(bayesplot)
#library(rstanarm)

# coefficient plot
plot(model_reg_8_B)
plot(model_reg_9_B) 




# posterior interval
cutoff <- 0.001

pi_8_B <- posterior_interval(model_reg_8_B)

significant_8_B <- pi_8_B[!(pi_8_B[,1] <= 0 & pi_8_B[,2] >= 0) & (abs(pi_8_B[,1] + pi_8_B[,2])/2 > cutoff), ]
print("Model 8")
print(significant_8_B)



pi_9_B <- posterior_interval(model_reg_9_B)

significant_9_B <- pi_9_B[!(pi_9_B[,1] <= 0 & pi_9_B[,2] >= 0) & (abs(pi_9_B[,1] + pi_9_B[,2])/2 > cutoff), ]
print("Model 9")
print(significant_9_B)

```

Next we study the posterior UNCERTAINTY on the likelihood noise.
```{R}


sigma_posterior <- as.data.frame(model_reg_8_B)[, "sigma"]  # Extract the posterior samples of sigma
sigma_df8 <- data.frame(sigma = as.data.frame(model_reg_8_B)[, "sigma"])


sigma_df8 %>% ggplot( aes(x = sigma)) +
  geom_histogram(bins = 30, fill = "red") +
  ggtitle("Posterior Distribution of sigma") +
  xlab("sigma") +
  ylab("Density") +
  theme_minimal()


sigma_posterior <- as.data.frame(model_reg_9_B)[, "sigma"]  # Extract the posterior samples of sigma
sigma_df9 <- data.frame(sigma = as.data.frame(model_reg_9_B)[, "sigma"])


sigma_df9 %>% ggplot( aes(x = sigma)) +
  geom_histogram(bins = 30, fill = "red") +
  ggtitle("Posterior Distribution of sigma") +
  xlab("sigma") +
  ylab("Density") +
  theme_minimal()

```
In our case, the variance of $\sigma$ is relatively small, we do have precise value.



## Linear Model Predictions


We will pick model 8 for it is compatible with EDA and model 9 for its performance. We at first give a direct comparison on the prediction and testing results.

```{R}
#mod_reg_8 <- lm(response_logit ~  (poly(R, 2) + poly(G, 2) + poly(B, 2) + poly(Hue, 2)) * (Lightness + Saturation), data = training_reg)


# Can consider interactions of basis functions with other basis functions!
#mod_reg_9 <- lm(response_logit ~  (poly(R, 2) * poly(G, 2) + poly(B, 2) * poly(Hue, 2) + poly(R, 2) * poly(Hue, 2) + poly(B, 2) * poly(R, 2)  + poly(G, 2) * poly(Hue, 2) + poly(B, 2) * poly(G, 2))
#                , data = training_reg)

set.seed(123)
inTraining <- createDataPartition(df_train$response_logit, p=.95, list = FALSE)
training_reg <- df_train[inTraining,]
testing_reg  <- df_train[-inTraining,]


# prediction
predictions_reg_8 <- predict(mod_reg_8, newdata = testing_reg)
predictions_reg_9 <- predict(mod_reg_9, newdata = testing_reg)


ggplot(testing_reg, aes(x = response_logit)) +
  geom_point(aes(y = predictions_reg_8), color = 'blue', alpha = 0.5) +
  #geom_point(aes(y = predictions_reg_9), color = 'red', alpha = 0.5) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
  facet_wrap(~Lightness)+
    labs(title = "mod_reg_8",
       x = "Actual response_logit",
       y = "Predicted response_logit") 

ggplot(testing_reg, aes(x = response_logit)) +
  geom_point(aes(y = predictions_reg_9), color = 'blue', alpha = 0.5) +
  facet_wrap(~Lightness)+
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
  labs(title = "mod_reg_9",
       x = "Actual response_logit",
       y = "Predicted response_logit") 


```

Next, we include the predictive mean trend, the confidence interval on the mean, and the prediction interval on the  response.
```{R}

## Model 8
prediction_details <- mod_reg_8 %>% predict(newdata = testing_reg, type = "response", interval = "prediction", level = 0.95)


testing_reg$predicted_response = prediction_details[, "fit"]
testing_reg$lower_pi = prediction_details[, "lwr"]
testing_reg$upper_pi = prediction_details[, "upr"]


mean_predictions <- mod_reg_8 %>%  predict( newdata = testing_reg, type = "response", interval = "confidence", level = 0.95)
testing_reg$lower_ci = mean_predictions[, "lwr"]
testing_reg$upper_ci = mean_predictions[, "upr"]

ggplot(testing_reg, aes(x = response_logit, y = predicted_response)) +
  geom_point(color = 'blue', alpha = 0.8, size = 2) +
  #geom_line(aes(y = predicted_response), color = "blue") +
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci), alpha = 0.2, fill = "blue", linetype = "dotted") +
  geom_ribbon(aes(ymin = lower_pi, ymax = upper_pi), alpha = 0.1, fill = "red") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
  labs(title = "Comparison of Actual vs. Predicted response_logit (Model reg_8)",
       x = "Actual response_logit",
       y = "Predicted response_logit") +
  theme_minimal()


## Model 9

prediction_details <- mod_reg_9 %>% predict(newdata = testing_reg, type = "response", interval = "prediction", level = 0.95)


testing_reg$predicted_response = prediction_details[, "fit"]
testing_reg$lower_pi = prediction_details[, "lwr"]
testing_reg$upper_pi = prediction_details[, "upr"]


mean_predictions <- mod_reg_9 %>%  predict( newdata = testing_reg, type = "response", interval = "confidence", level = 0.95)
testing_reg$lower_ci = mean_predictions[, "lwr"]
testing_reg$upper_ci = mean_predictions[, "upr"]

ggplot(testing_reg, aes(x = response_logit, y = predicted_response)) +
  geom_point(color = 'blue', alpha = 0.8, size = 2) +
  #geom_line(aes(y = predicted_response), color = "blue") +
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci), alpha = 0.2, fill = "blue", linetype = "dotted") +
  geom_ribbon(aes(ymin = lower_pi, ymax = upper_pi), alpha = 0.1, fill = "red") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
  labs(title = "Comparison of Actual vs. Predicted response_logit (Model reg_9)",
       x = "Actual response_logit",
       y = "Predicted response_logit") +
  theme_minimal()
```



## Train/Tune with Resampling
1. Linear models
2. Regularized regression with Elastic net
3. Neural network
4. Random forest
5. Gradient boosted tree
6. K-nearest Neighborhoods
7.  Support Vector Regression 



We adopt ``RMSE`` as the training metric.
```{R}
library(caret)



# data set
set.seed(123)


df_train_reg <-dplyr::select(df_train, -outcome, -response)
df_train_cla <-dplyr::select(df_train, -response_logit, -response)

inTraining <- createDataPartition(df_train_reg$response_logit, p=.95, list = FALSE)

training <- df_train_reg[inTraining,]
testing  <- df_train_reg[-inTraining,]


fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 5,
                           ## repeated ten times
                           repeats = 3)

my_metrics_regress <- 'RMSE'

#Linear Models

# All categorical and continuous inputs - linear additive features
caret_reg_1_1 <- train(response_logit ~ ., 
                     data = training, 
                     method = "lm",
                     metric = my_metrics_regress,
                     trControl = fitControl)

# Add categorical inputs to all main effect
# and all pairwise interactions of continuous inputs

caret_reg_1_2 <- train(response_logit ~ (R + G + B + Hue)^2 + Lightness + Saturation, 
                       data = training, 
                       method = "lm",
                       metric = my_metrics_regress,
                       trControl = fitControl)

#mod_reg_8 <- lm(response_logit ~  (poly(R, 2) + poly(G, 2) + poly(B, 2) + poly(Hue, 2)) * (Lightness + Saturation), data = training_reg)

caret_reg_1_3 <- train(response_logit ~ (R  + G + B + I(R^2) + I(G^2) + I(B^2) + Hue + I(Hue^2)) * 
                         (Lightness + Saturation), 
                       data = training, 
                       method = "lm",
                       metric = my_metrics_regress,
                       trControl = fitControl)

# Can consider interactions of basis functions with other basis functions!
#mod_reg_9 <- lm(response_logit ~  (poly(R, 2) * poly(G, 2) + poly(B, 2) * poly(Hue, 2) + poly(R, 2) * poly(Hue, 2) + poly(B, 2) * poly(R, 2)  + poly(G, 2) * poly(Hue, 2) + poly(B, 2) * poly(G, 2))
#                , data = training_reg)

caret_reg_1_4 <- train(response_logit ~  ((R+I(R^2)) * ((B+I(B^2))+(G+I(G^2))+(Hue + I(Hue^2))) + 
                                        ((B+I(B^2)) * ((G+I(G^2))+(Hue + I(Hue^2))) +
                                                      (G+I(G^2)) *(Hue + I(Hue^2)) ) ), 
                       data = training, 
                       method = "lm",
                       metric = my_metrics_regress,
                       trControl = fitControl)


# Regularized Regression with Elastic Net

# Define a tuning grid
grid_elastic <- expand.grid(alpha = seq(0, 1, length = 5), 
                            lambda = seq(0.001, 0.1, length = 5))

# Elastic net model

# Add categorical inputs to all main effect 
# and all pairwise interactions of continuous inputs
caret_reg_2_1 <- train(response_logit ~ (R + G + B + Hue)^2 + Lightness + Saturation, 
                     data = training, 
                     method = "glmnet",
                     metric = my_metrics_regress,
                     tuneGrid = grid_elastic,
                     trControl = fitControl)

#mod_reg_9 <- lm(response_logit ~  (poly(R, 2) * poly(G, 2) + poly(B, 2) * poly(Hue, 2) + poly(R, 2) * poly(Hue, 2) + poly(B, 2) * poly(R, 2)  + poly(G, 2) * poly(Hue, 2) + poly(B, 2) * poly(G, 2))
#                , data = training_reg)


caret_reg_2_2 <- train(response_logit ~  ((R+I(R^2)) * ((B+I(B^2))+(G+I(G^2))+(Hue + I(Hue^2))) + 
                                        ((B+I(B^2)) * ((G+I(G^2))+(Hue + I(Hue^2))) +
                                                      (G+I(G^2)) *(Hue + I(Hue^2)) ) ), 
                     data = training, 
                     method = "glmnet",
                     metric = my_metrics_regress,
                     tuneGrid = grid_elastic,
                     trControl = fitControl)



# Neural Network 
caret_reg_3 <- train(response_logit ~ ., 
                     data = training, 
                     method = "nnet",
                     metric = my_metrics_regress,
                     trControl = fitControl,
                     tuneLength = 5,
                     trace = FALSE)


# Random Forest 

# method = "rf", not working
#caret_reg_4 <- train(response_logit ~ ., 
#                     data = training, 
#                     method = "rf",
#                     trControl = fitControl,
#                     tuneLength = 3) 

caret_reg_4 <- train(response_logit ~ ., 
                     data = training, 
                     method = "ranger",
                     metric = my_metrics_regress,
                     trControl = fitControl,
                     tuneLength = 3,
                     importance = 'impurity', # set feature importance to use varImp()
                     verbose = FALSE)


# Gradient Boosted Tree
caret_reg_5 <- train(response_logit ~ ., 
                 data = training, 
                 method = "gbm",
                 metric = my_metrics_regress,
                 trControl = fitControl,
                 ## for gbm()  passing through
                 verbose = FALSE)


# K-nearest Neighbors
caret_reg_6 <- train(response_logit ~ ., 
                     data = training, 
                     method = "knn",
                     metric = my_metrics_regress,
                     trControl = fitControl,
                     tuneLength = 5)



# Support Vector Regression 
caret_reg_7 <- train(response_logit ~ ., 
                   data = training, 
                   method = "svmRadial",  
                   metric = my_metrics_regress,
                   trControl = fitControl,
                   preProcess = c("center", "scale"),  
                   tuneLength = 5)
```






```{R}
# Visualization

#caret_models <- list(caret_reg_1_1, caret_reg_1_2, caret_reg_1_3, caret_reg_1_4, caret_reg_2_1, caret_reg_2_2, 
#caret_reg_3, caret_reg_4, caret_reg_5, caret_reg_6, caret_reg_7)
model_list <- list(
  caret_reg_1_1 = caret_reg_1_1,
  caret_reg_1_2 = caret_reg_1_2,
  caret_reg_1_3 = caret_reg_1_3,
  caret_reg_1_4 = caret_reg_1_4,
  caret_reg_2_1 = caret_reg_2_1,
  caret_reg_2_2 = caret_reg_2_2,
  caret_reg_3 = caret_reg_3,
  caret_reg_4 = caret_reg_4,
  caret_reg_5 = caret_reg_5,
  caret_reg_6 = caret_reg_6,
  caret_reg_7 = caret_reg_7
)


# model 1_1 to 3
for(model_name in names(model_list[1:7])) {
  importance <- varImp(model_list[[model_name]], 
                       scale = FALSE)
  print(
    plot(importance) 
  )
}


```




Next, we resample and plot the metrics for each model.
```{R}
resamps <- resamples(list(
  lm_1_1 = caret_reg_1_1,
  lm_1_2 = caret_reg_1_2,
  lm_1_3 = caret_reg_1_3,
  lm_1_4 = caret_reg_1_4,
  glmnet_2_1 = caret_reg_2_1,
  glmnet_2_2 = caret_reg_2_2,
  nnt_3 = caret_reg_3,
  ranger_4 = caret_reg_4,
  gbt_5 = caret_reg_5,
  knn_6 = caret_reg_6,
  svr_7 = caret_reg_7))
resamps %>% summary()


#trellis.par.set(caretTheme())
dotplot(resamps)

```

Apparently, the best performers are our linear model 9 and 8 trained from ``lm()``.








# Classification


## General Linear Model

- intercept-only model – no INPUTS!

- Categorical variables only – linear additive

- Continuous variables only – linear additive

- All categorical and continuous variables – linear additive

- Interaction of the categorical inputs with all continuous inputs main effects

- Add categorical inputs to all main effect and all pairwise interactions of continuous inputs

- Interaction of the categorical inputs with all main effect and all pairwise interactions of continuous inputs

- 3 models with basis functions of your choice


We will answer the following questions accordingly: Which of the 10 models is the best?  What performance metric did you use to make your selection? Visualize the coefficient summaries for your top 3 models. Which inputs seem important?



```{R}

set.seed(123)
inTraining <- createDataPartition(df_train$outcome, p=.95, list = FALSE)
training_cla <- df_train[inTraining,]
testing_cla  <- df_train[-inTraining,]



# Intercept-only model – no INPUTS!
mod_cla_1 <- glm(outcome ~ 1, data = training_cla, family = binomial)


# Categorical variables only – linear additive
mod_cla_2 <- glm(outcome ~ Lightness + Saturation, data = training_cla, family = binomial)

# Continuous variables only – linear additive
mod_cla_3 <- glm(outcome ~ R + G + B + Hue, data = training_cla, family = binomial)

# All categorical and continuous variables – linear additive
mod_cla_4 <- glm(outcome ~ Lightness + Saturation + R + G + B + Hue, data = training_cla, family = binomial)

# Interaction of the categorical inputs with all continuous inputs main effects
mod_cla_5 <- glm(outcome ~ (Lightness + Saturation) : (R + G + B + Hue), data = training_cla, family = binomial)

# Add categorical inputs to all main effect and all pairwise interactions of continuous inputs
mod_cla_6 <- glm(outcome ~ Lightness + Saturation + R + G + B + Hue + 
                R:G + R:B + R:Hue + G:B + G:Hue + B:Hue, data = training_cla, family = binomial)

# Interaction of the categorical inputs with all main effect and all pairwise interactions of continuous inputs
mod_cla_7 <- glm(outcome ~ (Lightness + Saturation) *
                (R + G + B + Hue + R:G + R:B + R:Hue ),
                data = training_cla, family = binomial)


# Try non-linear basis functions based on your EDA
mod_cla_8 <- glm(outcome ~ B + R + G + Hue +
                   (B+R+G) : ( poly(Hue, 2) + Lightness + Saturation) , data = training_cla, family = binomial)

# Can consider interactions of basis functions with other basis functions!

mod_cla_9 <- glm(outcome ~ Lightness + Saturation + Lightness:(R + G + Hue) + Saturation:(B+ G) + B:poly(Hue, 2) ,
                 data = training_cla, family = binomial)




# Can consider interactions of basis functions with the categorical inputs!
mod_cla_10 <- glm(outcome ~  R : (G+B + poly(Hue, 2) + Lightness + Saturation) + 
                                G : (B + poly(Hue, 2) + Lightness + Saturation) + 
                                B : ((poly(Hue, 2)) + Lightness + Saturation)
                , data = training_cla, family = binomial)


```
For Model 1 to 7, they are the same models from the regression problem.
Some model like mod_cla_7, had warning:  ``fitted probabilities numerically 0 or 1 occurred``.



We now evaluate our general linear models with AIC, BIC metrics.
```{R}
library(broom)

model_list <- list(
  mod_cla_1, mod_cla_2, mod_cla_3, mod_cla_4, mod_cla_5, 
  mod_cla_6, mod_cla_7, mod_cla_8, mod_cla_9, mod_cla_10
)
names(model_list) <- paste("Model", 1:10)

model_summaries <- lapply(names(model_list), function(model_name) {
  model_summary <- glance(model_list[[model_name]])
  model_summary$model <- model_name
  return(model_summary)
})


model_summaries_df <- bind_rows(model_summaries)
model_summaries_df$model <- factor(model_summaries_df$model,
                                   levels = paste("Model", 1:10))

# Plotting
model_summaries_df %>%  ggplot( aes(x = model)) +
  geom_point(aes(y = AIC, color = "AIC")) + 
  geom_line(aes(y = AIC, group = 1, color = "AIC")) + 
  geom_point(aes(y = BIC, color = "BIC")) + 
  geom_line(aes(y = BIC, group = 1, color = "BIC")) +
  theme_minimal() 



model_summaries_df %>%  arrange(AIC) %>% print()
model_summaries_df %>%  arrange(BIC) %>% print()


```
From AIC metric, Model 9, 8 and 10 are the top 3 performers.


Next, we make an attempt to plot the coefficients of our models. 
```{R}
# Plotting Coefficients

#coefplot(mod_cla_10, intercept = TRUE)  + 
#  labs(title = 'Coefficient Plot for Model 10')
 
coefplot(mod_cla_9, intercept = TRUE)  + 
  labs(title = 'Coefficient Plot for Model 9')

coefplot(mod_cla_8, intercept = TRUE)  + 
  labs(title = 'Coefficient Plot for Model 8')
```
From ``coefplot::coefplot``, the graphs are unreadable due to the complexity of models. We do the following to simplify.



```{R}
library(dplyr)


# mod 10
coef_10 <- summary(mod_cla_10)$coefficients %>%
  as.data.frame() %>%  
  rownames_to_column(var = "Term") %>%  
  as_tibble() %>% 
  mutate(
    lower_bound = Estimate - 1.96 * `Std. Error`,
    upper_bound = Estimate + 1.96 * `Std. Error` 
  ) %>%
  dplyr::select(Term, Estimate, Std.Error = `Std. Error`, lower_bound, upper_bound) 


significant_coefs_10 <- coef_10 %>%
  filter(lower_bound > 0 | upper_bound < 0) 

significant_coefs_10 %>%  ggplot( aes(x = Term, y = Estimate, ymin = lower_bound, ymax = upper_bound)) +
  geom_pointrange() +  
  coord_flip() +  
  labs(title = "Significant Coefficients Model 10", x = "Term", y = "Estimate") +
  theme_minimal() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red")  





# mod 9

coef_9 <- summary(mod_cla_9)$coefficients %>%
  as.data.frame() %>%  
  rownames_to_column(var = "Term") %>%  
  as_tibble() %>% 
  mutate(
    lower_bound = Estimate - 1.96 * `Std. Error`,
    upper_bound = Estimate + 1.96 * `Std. Error` 
  ) %>%
  dplyr::select(Term, Estimate, Std.Error = `Std. Error`, lower_bound, upper_bound) 


significant_coefs_9 <- coef_9 %>%
  filter(lower_bound > 0 | upper_bound < 0) 

significant_coefs_9 %>%  ggplot( aes(x = Term, y = Estimate, ymin = lower_bound, ymax = upper_bound)) +
  geom_pointrange() +  
  coord_flip() +  
  labs(title = "Significant Coefficients Model 9", x = "Term", y = "Estimate") +
  theme_minimal() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red")  


# mod 8

coef_8 <- summary(mod_cla_8)$coefficients %>%
  as.data.frame() %>%  
  rownames_to_column(var = "Term") %>%  
  as_tibble() %>% 
  mutate(
    lower_bound = Estimate - 1.96 * `Std. Error`,
    upper_bound = Estimate + 1.96 * `Std. Error` 
  ) %>%
  dplyr::select(Term, Estimate, Std.Error = `Std. Error`, lower_bound, upper_bound) 


significant_coefs_8 <- coef_8 %>%
  filter(lower_bound > 0 | upper_bound < 0) 

significant_coefs_8 %>%  ggplot( aes(x = Term, y = Estimate, ymin = lower_bound, ymax = upper_bound)) +
  geom_pointrange() +  
  coord_flip() +  
  labs(title = "Significant Coefficients Model 8", x = "Term", y = "Estimate") +
  theme_minimal() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red")  



```
Throughout the 3 models, we notice that ``lightness`` and its interaction with other variable is extremely essential.


## Baysian GLM

We use the Laplace Approximation approach and define the following function to apply Bayesian methods.
We will use general linear models 9 and 8. Model 9 is the best performer from AIC and BIC; model 8 is the second best performer constructed from EDA.

```{r, logpost}
logistic_logpost <- function(unknowns, my_info)
{
  # extract the design matrix and assign to X
  X <- my_info$design_matrix
  
  # calculate the linear predictor
  eta <- X %*% unknowns
  
  # calculate the event probability
  mu <- 1 / (1 + exp(-eta))
  
  # evaluate the log-likelihood
  y <- my_info$yobs
  log_lik <- sum(y * log(mu) + (1 - y) * log(1 - mu))
   
  
  # evaluate the log-prior
  mu_beta <- my_info$mu_beta
  tau_beta <- my_info$tau_beta
  log_prior <- sum(dnorm(unknowns, mean=mu_beta,
                         sd=tau_beta, log=TRUE))
  
  # sum together
  log_post <- log_lik + log_prior
  return(log_post)
}



```


```{r, define_my_laplace_func, eval=TRUE}
my_laplace <- function(start_guess, logpost_func, ...)
{
  # code adapted from the `LearnBayes`` function `laplace()`
  fit <- optim(start_guess,
               logpost_func,
               gr = NULL,
               ...,
               method = "BFGS",
               hessian = TRUE,
               control = list(fnscale = -1, maxit = 5001))
  
  mode <- fit$par
  post_var_matrix <- -solve(fit$hessian)
  p <- length(mode)
  int <- p/2 * log(2 * pi) + 0.5 * log(det(post_var_matrix)) + logpost_func(mode, ...)
  # package all of the results into a list
  list(mode = mode,
       var_matrix = post_var_matrix,
       log_evidence = int,
       converge = ifelse(fit$convergence == 0,
                         "YES", 
                         "NO"),
       iter_counts = as.numeric(fit$counts[1]))
}
```

```{R}
#mod_cla_9 <- glm(outcome ~ Lightness + Saturation + Lightness:(R + G + Hue) + Saturation:(B+ G) + B: (Hue + Hue2) ,
#                 data = training_cla, family = binomial)



#df_train_B


Xmat_9 <- model.matrix(~ Lightness + Saturation+ Lightness:(R + G + Hue) + Saturation:(B+ G)+  (Hue+Hue2),#:B,
                       data=df_train_B)

info_9 <- list(
  yobs = df_train_B$outcome,
  design_matrix = Xmat_9,
  mu_beta = 0,
  tau_beta = 4.5
)

initial_guess_9 <- rep(0, ncol(Xmat_9))

laplace_9 <- my_laplace(initial_guess_9, logistic_logpost, my_info=info_9)




#mod_cla_8 <- glm(outcome ~ B + R + G + Hue 
#                 + (B+R+G) : ( Hue + Hue2 + Lightness + Saturation) , 
#                 data = training_cla, family = binomial)


# compute Laplace
Xmat_8 <- model.matrix(~  B + R + G + Hue + 
                      Hue2  + #(B+R+G) : ( Hue + Hue2) + 
                         (B+R+G):(Lightness + Saturation),
                       data=df_train_B)

info_8 <- list(
  yobs = df_train_B$outcome,
  design_matrix = Xmat_8,
  mu_beta = 0,
  tau_beta = 4.5
)

initial_guess_8 <- rep(0, ncol(Xmat_8))

laplace_8 <- my_laplace(initial_guess_8, logistic_logpost, my_info=info_8)



```


```{R}

evidences <- c(laplace_8$log_evidence,
               laplace_9$log_evidence)


weights_unnormalized <- exp(evidences - max(evidences))
weights <- weights_unnormalized / sum(weights_unnormalized)


model_weights_df <- data.frame(
  Model = c('Bayesian Model 8', 'Bayesian Model 9'),
  Weight = weights
)


ggplot(model_weights_df, aes(x = Model, y = Weight)) +
  geom_bar(stat = "identity") +
  theme_minimal() 

```
From the weight of models, Model 9 is the best.


To plot the significant coefficients:
```{R}

# Model 9


std_errors <- sqrt(diag(laplace_9$var_matrix))

ci_lower <- laplace_9$mode - 1.96 * std_errors
ci_upper <- laplace_9$mode + 1.96 * std_errors

ci_bounds <- cbind(lower_bound = ci_lower, upper_bound = ci_upper,  mode = laplace_9$mode)

coef_9_B <- ci_bounds %>%
  as.data.frame() %>%  
  rownames_to_column() %>%  
  as_tibble() 


significant_coefs_9_B <- coef_9_B %>%
  filter(lower_bound > 0 | upper_bound < 0) 

significant_coefs_9_B %>%  ggplot( aes(x = rowname, y = mode, ymin = lower_bound, ymax = upper_bound)) +
  geom_pointrange() +  
  coord_flip() +  
  labs(title = "Significant Coefficients Baysian Model 9 ", x = "Term", y = "Estimate") +
  theme_minimal() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red")  



# Model 8



std_errors <- sqrt(diag(laplace_8$var_matrix))

ci_lower <- laplace_8$mode - 1.96 * std_errors
ci_upper <- laplace_8$mode + 1.96 * std_errors

ci_bounds <- cbind(lower_bound = ci_lower, upper_bound = ci_upper,  mode = laplace_8$mode)

coef_8_B <- ci_bounds %>%
  as.data.frame() %>%  
  rownames_to_column() %>%  
  as_tibble() 


significant_coefs_8_B <- coef_8_B %>%
  filter(lower_bound > 0 | upper_bound < 0) 

significant_coefs_8_B %>%  ggplot( aes(x = rowname, y = mode, ymin = lower_bound, ymax = upper_bound)) +
  geom_pointrange() +  
  coord_flip() +  
  labs(title = "Significant Coefficients Baysian Model 8 ", x = "Term", y = "Estimate") +
  theme_minimal() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red")  
```



## GLM Predictions

Now we make direct prediction from the best 2 models, model 8 and 9.


```{R}

set.seed(123)
inTraining <- createDataPartition(df_train$outcome, p=.95, list = FALSE)
training_cla <- df_train[inTraining,]
testing_cla  <- df_train[-inTraining,]




# Model 8
testing_cla$pred_proba_8 <- predict(mod_cla_8, newdata = testing_cla, type = "response")


ggplot(testing_cla, aes(x = pred_proba_8, fill = as.factor(outcome))) +
  geom_histogram(data = testing_cla, binwidth = 0.2, alpha = 0.5, position = "fill") +
    #facet_wrap(~Lightness) +
  labs(title = "mod_cla_8",
       x = "Predicted Probability",
       y = "Percentage",
       fill = "Actual Outcome") +
  theme_minimal()

# Plot for model 9


testing_cla$pred_proba_9 <- predict(mod_cla_9, newdata = testing_cla, type = "response")


ggplot(testing_cla, aes(x = pred_proba_9, fill = as.factor(outcome))) +
  geom_histogram(data = testing_cla, binwidth = 0.25, alpha = 0.5, position = "fill") +
  labs(title = "mod_cla_9",
       x = "Predicted Probability",
       y = "Percentage",
       fill = "Actual Outcome") +
  theme_minimal()


```






We also can find their ROC value.
```{R}

library(pROC)

# model 8
roc_response_8 <- roc(testing_cla$outcome, testing_cla$pred_proba_8)
plot(roc_response_8, main="ROC for mod_cla_8")


# model 9
roc_response_9 <- roc(testing_cla$outcome, testing_cla$pred_proba_9)
plot(roc_response_9, main="ROC for mod_cla_9")



print("model 8")
auc(roc_response_8)
print("model 9")
auc(roc_response_9)


```



## Train/Tune with resampling

We now use ``caret`` to train more models.

```{R, eval=FALSE}
library(caret)



# data set
set.seed(123)


df_train_reg <- dplyr::select(df_train, -outcome, -response)
df_train_cla <- dplyr::select(df_train, -response_logit, -response) %>% 
  mutate(outcome = ifelse(outcome == 1, 'event', 'non_event'),
         outcome = factor(outcome, levels = c('event', 'non_event')))

inTraining <- createDataPartition(df_train_cla$outcome, p=.9, list = FALSE)


training <- df_train_cla[inTraining,]

testing  <- df_train_cla[-inTraining,]


fitControl <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated 3 times
                           repeats = 5,
                           classProbs = TRUE
                           )


#my_binary_method <- 'glm'
#my_preProcess <- c('center', 'scale')
#mod_binary_acc <- train(outcome ~ Hue + Lightness,
#                        data = dfiiiD,
#                        method = my_binary_method,
#                        preProcess = c('center', 'scale'),
#                        metric = my_metrics_acc,
#                        trControl = my_ctrl_acc)

my_metrics_cla <- 'Accuracy'
my_preProcess <- c('center', 'scale')


#Linear Models

# All categorical and continuous inputs - linear additive features
caret_cla_1_1 <- train(outcome ~ ., 
                     data = training, 
                     method = "glm",
                     metric = my_metrics_cla,
                     #my_preProcess = c('center', 'scale'),
                     trControl = fitControl)

#caret_cla_1_1 %>% readr::write_rds("model_caret_cla_1_1.rds")
#caret_cla_1_1 <- readr::read_rds("model_caret_cla_1_1.rds")

# Add categorical inputs to all main effect
# and all pairwise interactions of continuous inputs

caret_cla_1_2 <- train(outcome ~ (R + G + B + Hue)^2 + Lightness + Saturation, 
                       data = training, 
                       method = "glm",
                       metric = my_metrics_cla,
                       trControl = fitControl)



#mod_cla_8 <- glm(outcome ~ B + R + G + Hue +
#                   (B+R+G) : ( poly(Hue, 2) + Lightness + Saturation), data = training_cla, family = binomial)



caret_cla_1_3 <- train(outcome ~ B + R + G + Hue +
                   (B+R+G) : ( Hue + I(Hue^2) + Lightness + Saturation), 
                       data = training, 
                       method = "glm",
                       metric = my_metrics_cla,
                       trControl = fitControl)

#mod_cla_10 <- glm(outcome ~  R * (G+B + poly(Hue, 2) + Lightness + Saturation) + 
#                                G * (B + poly(Hue, 2) + Lightness + Saturation) + 
#                                B *( (poly(Hue, 2)) + Lightness + Saturation)
#                , data = training_cla, family = binomial)             , data = training_reg)

caret_cla_1_4 <- train(outcome ~  R * (G+B + Hue + I(Hue^2) + Lightness + Saturation) + 
                                G * (B + Hue + I(Hue^2)+ Lightness + Saturation) + 
                                B *( Hue + I(Hue^2) + Lightness + Saturation),
                       data = training, 
                       method = "glm",
                       metric = my_metrics_cla,
                       trControl = fitControl)







# Regularized Regression with Elastic Net

# Define a tuning grid
grid_elastic <- expand.grid(alpha = seq(0, 1, length = 5), 
                            lambda = seq(0.001, 0.1, length = 5))

# Elastic net model

# Add categorical inputs to all main effect 
# and all pairwise interactions of continuous inputs
caret_cla_2_1 <- train(outcome ~ (R + G + B + Hue)^2 + Lightness + Saturation, 
                     data = training, 
                     method = "glmnet",
                     metric = my_metrics_cla,
                     tuneGrid = grid_elastic,
                     trControl = fitControl)

#mod_reg_9 <- lm(response_logit ~  (poly(R, 2) * poly(G, 2) + poly(B, 2) * poly(Hue, 2) + poly(R, 2) * poly(Hue, 2) + poly(B, 2) * poly(R, 2)  + poly(G, 2) * poly(Hue, 2) + poly(B, 2) * poly(G, 2))
#                , data = training_reg)


caret_cla_2_2 <- train(outcome ~ R * (G+B + Hue + I(Hue^2) + Lightness + Saturation) + 
                                G * (B + Hue + I(Hue^2)+ Lightness + Saturation) + 
                                B *( Hue + I(Hue^2) + Lightness + Saturation),
                     data = training, 
                     method = "glmnet",
                     metric = my_metrics_cla,
                     tuneGrid = grid_elastic,
                     trControl = fitControl)







# Neural Network 
caret_cla_3 <- train(outcome ~ ., 
                     data = training, 
                     method = "nnet",
                     metric = my_metrics_cla,
                     trControl = fitControl,
                     tuneLength = 5,
                     trace = FALSE)


# Random Forest 

# method = "rf", not working
#caret_reg_4 <- train(response_logit ~ ., 
#                     data = training, 
#                     method = "rf",
#                     trControl = fitControl,
#                     tuneLength = 3) 
#
#training <- df_train_cla
#fitControl <- trainControl(## 5-fold CV
#                           method = "repeatedcv",
#                           number = 10,
#                           ## repeated 7 times
#                           repeats = 7,
#                           classProbs = TRUE
#                           )


caret_cla_4 <- train(outcome ~ ., 
                     data = training, 
                     method = "ranger",
                     metric = my_metrics_cla,
                     trControl = fitControl,
                     tuneLength = 7,
                     importance = 'impurity', # set feature importance to use varImp()
                     verbose = FALSE)


# Gradient Boosted Tree


caret_cla_5 <- train(outcome ~ ., 
                 data = training, 
                 method = "gbm",
                 metric = my_metrics_cla,
                 trControl = fitControl,
                 ## for gbm()  passing through
                 verbose = FALSE)


# K-nearest Neighbors
caret_cla_6 <- train(outcome ~ ., 
                     data = training, 
                     method = "knn",
                     metric = my_metrics_cla,
                     trControl = fitControl,
                     tuneLength = 5)



# Support Vector Regression 
caret_cla_7 <- train(outcome ~ ., 
                   data = training, 
                   method = "svmRadial",  
                   metric = my_metrics_cla,
                   trControl = fitControl,
                   preProcess = c("center", "scale"),  
                   tuneLength = 5)

```

```{R}
# model management

caret_cla_1_1 <- readr::read_rds("model_caret_cla_1_1.rds")
caret_cla_1_2 <- readr::read_rds("model_caret_cla_1_2.rds")
caret_cla_1_3 <- readr::read_rds("model_caret_cla_1_3.rds")
caret_cla_1_4 <- readr::read_rds("model_caret_cla_1_4.rds")
caret_cla_2_1 <- readr::read_rds("model_caret_cla_2_1.rds")
caret_cla_2_2 <- readr::read_rds("model_caret_cla_2_2.rds")
caret_cla_3 <- readr::read_rds("model_caret_cla_3.rds")
caret_cla_4 <- readr::read_rds("model_caret_cla_4.rds")
caret_cla_5 <- readr::read_rds("model_caret_cla_5.rds")
caret_cla_6 <- readr::read_rds("model_caret_cla_6.rds")
caret_cla_7 <- readr::read_rds("model_caret_cla_7.rds")




#caret_cla_1_1 %>% readr::write_rds("model_caret_cla_1_1.rds")
#caret_cla_1_2 %>% readr::write_rds("model_caret_cla_1_2.rds")
#caret_cla_1_3 %>% readr::write_rds("model_caret_cla_1_3.rds")
#caret_cla_1_4 %>% readr::write_rds("model_caret_cla_1_4.rds")
#caret_cla_2_1 %>% readr::write_rds("model_caret_cla_2_1.rds")
#caret_cla_2_2 %>% readr::write_rds("model_caret_cla_2_2.rds")
#caret_cla_3 %>% readr::write_rds("model_caret_cla_3.rds")
#caret_cla_4 %>% readr::write_rds("model_caret_cla_4.rds")
#caret_cla_5 %>% readr::write_rds("model_caret_cla_5.rds")
#caret_cla_4 %>% readr::write_rds("model_caret_cla_4_enhanced.rds")
#caret_cla_5 %>% readr::write_rds("model_caret_cla_5_enhanced.rds")
#caret_cla_6 %>% readr::write_rds("model_caret_cla_6.rds")
#caret_cla_7 %>% readr::write_rds("model_caret_cla_7.rds")


```

We perform resamplig and compare the models above via different metrics.

```{R}

resamps_cla <- resamples(list(
  glm_1_1 = caret_cla_1_1,
  glm_1_2 = caret_cla_1_2,
  glm_1_3 = caret_cla_1_3,
  glm_1_4 = caret_cla_1_4,
  glmnet_2_1 = caret_cla_2_1,
  glmnet_2_2 = caret_cla_2_2,
  nnt_3 = caret_cla_3,
  ranger_4 = caret_cla_4,
  gbt_5 = caret_cla_5,
  knn_6 = caret_cla_6,
  svr_7 = caret_cla_7))


resamps_cla %>% summary()


#trellis.par.set(caretTheme())
dotplot(resamps_cla)



```
By Accuracy, Random forest and  Gradient boosted tree have the best performance.




To verify the effects of the top performers, we plot the prediction on testing set.
```{R, caret classification prediction}
# Plot the prediction

df_train_reg <- dplyr::select(df_train, -outcome, -response)
df_train_cla <- dplyr::select(df_train, -response_logit, -response) %>% 
  mutate(outcome = ifelse(outcome == 1, 'event', 'non_event'),
         outcome = factor(outcome, levels = c('event', 'non_event')))

inTraining <- createDataPartition(df_train_cla$outcome, p=.9, list = FALSE)

training <- df_train_cla[inTraining,]
testing  <- df_train_cla[-inTraining,]

# m4
testing_cla$pred_proba_4 <- predict(caret_cla_4, newdata = testing_cla, type = "prob")[, "event"]


ggplot(testing_cla, aes(x = pred_proba_4, fill = as.factor(outcome))) +
  geom_histogram(binwidth = 0.1, alpha = 0.5, position = "fill") +  # Adjusted binwidth if needed
  labs(title = "Predicted Probability Distribution for Model caret_cla_4",
       x = "Predicted Probability of Event",
       y = "Percentage",
       fill = "Actual Outcome") +
  theme_minimal()


# m5

testing_cla$pred_proba_5 <- predict(caret_cla_5, newdata = testing_cla, type = "prob")[, "event"]

ggplot(testing_cla, aes(x = pred_proba_5, fill = as.factor(outcome))) +
  geom_histogram(binwidth = 0.1, alpha = 0.5, position = "fill") +  # Adjusted binwidth if needed
  labs(title = "Predicted Probability Distribution for Model caret_cla_5",
       x = "Predicted Probability of Event",
       y = "Percentage",
       fill = "Actual Outcome") +
  theme_minimal()


#Density Plots
#densityplot(caret_cla_4, pch = "|", main="Model 4")
#densityplot(caret_cla_5, pch = "|", main="Model 5")


```


In the meantime, we plot the ROC.
```{R}
library(pROC)

set.seed(123)
inTraining <- createDataPartition(df_train$outcome, p=.95, list = FALSE)
training_cla <- df_train[inTraining,]
testing_cla  <- df_train[-inTraining,]


# Model 4
testing_cla$pred_proba_4 <- predict(caret_cla_4, newdata = testing_cla, type = "prob")[,"event"]

roc_response_4 <- roc(testing_cla$outcome, testing_cla$pred_proba_4)
plot(roc_response_4, main="ROC for caret_cla_4 ranger")


# model 5
testing_cla$pred_proba_5 <- predict(caret_cla_5, newdata = testing_cla, type = "prob")[,"event"]

roc_response_5 <- roc(testing_cla$outcome, testing_cla$pred_proba_5)
plot(roc_response_5, main="ROC for caret_cla_5 gbm")



print("model 4")
auc(roc_response_4)
print("model 5")
auc(roc_response_5)

```
Random forest seems to be the best if we are interested in maximizing Accuracy. By nature, random forest works well with large data set by increasing the maximal depth.
